{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Workflow:\n",
    "\n",
    "1. Problem description, data characteristics, visualisation, distributions, etc.\n",
    "\n",
    "2. Preprocessing:\n",
    "   - missing data (delete, leave, imputation + explaination);\n",
    "   - normalization/standarization (type of transformation + description, transformation categorical data);\n",
    "   - separation - test and training sets (training/test, cross-validation, proportions of subsets);\n",
    "\n",
    "3. Building and training neural network:\n",
    "    - layers, neurons quantity; \n",
    "    - activation functions (+description, last layer should transform to binary output);\n",
    "    - optimalisation method (+description, which one returns best results);\n",
    "    - loss function and metrics (+description);\n",
    "    - model fitting (epochs number, batch size, model evaluation (training result));\n",
    "    - test set based classification (evaluation methods of binary classifier, including data type (should include just and only most desriptive metrics\n",
    "\n",
    "4. Additional methods not presented in labs/lectures.\n",
    "\n",
    "5. Present different parameters values, to decide, which ones caused accuracy and classification improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yellowbrick\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy as sp\n",
    "import keras\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import seaborn as sns\n",
    "from keras import metrics as k_metrics\n",
    "from sklearn import metrics as s_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"logreg.txt\", sep = \";\", dtype={'genotype': np.object})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()\n",
    "# data.shape\n",
    "# print(data.dtypes)\n",
    "\n",
    "# data.BEFORE1.value_counts()\n",
    "# data.BEFORE2.value_counts()\n",
    "# data.BEFORE3.value_counts()\n",
    "\n",
    "# data.BEHIND1.value_counts()\n",
    "# data.BEHIND2.value_counts()\n",
    "# data.BEHIND3.value_counts()\n",
    "\n",
    "# descriptions_dataframe = data.describe(include='all')\n",
    "# nulls = data.isnull().sum().to_frame(name='NA').T \n",
    "# summary_of_nulls = pd.concat([descriptions_dataframe, nulls])\n",
    "# summary_of_nulls\n",
    "\n",
    "# data.iloc[:,6:].apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims = (15.2, 10.75)\n",
    "# fig, ax = plt.subplots(figsize=dims)\n",
    "\n",
    "# bpl = sns.boxplot(\n",
    "#     data=data,\n",
    "#     x='genotype',\n",
    "#     y='CALL',\n",
    "# )\n",
    "\n",
    "# bpl.figure.savefig(r\"C:\\Users\\Acer\\Desktop\\Studia\\Deep learning\\project\\SNP-predictions\\boxplots\\boxplot_genotype_CALL.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Creating triplets - we can't analyse categorical data (nucleotides: A, T, G, C, N), so we must find numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['BEFORE'] = data.BEFORE1 + data.BEFORE2 + data.BEFORE3\n",
    "tmp['BEHIND'] = data.BEHIND1 + data.BEHIND2 + data.BEHIND3\n",
    "data = data.drop(columns=['BEFORE1', 'BEFORE2', 'BEFORE3', 'BEHIND1', 'BEHIND2', 'BEHIND3'])\n",
    "\n",
    "tmp = pd.get_dummies(tmp)\n",
    "data = pd.concat([data, tmp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Histograms to check distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dims = (15.2, 10.75)\n",
    "# fig, ax = plt.subplots(figsize=dims)\n",
    "# sns_plot = sns.distplot(data['GQ'], ax = ax, hist=True, kde=True, \n",
    "#              bins=int(180/5), color = 'darkblue', \n",
    "#              hist_kws={'edgecolor':'black'},\n",
    "#              kde_kws={'linewidth': 4})\n",
    "\n",
    "# sns_plot.figure.savefig(r\"C:\\Users\\Acer\\Desktop\\Programs\\Python\\Deep learning\\project\\SNP-predictions\\Distribution plots\\dist_GQ.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Split - training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = data.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Missing data handling - imputation using kNN algorithm, Iterative Imputer, Simple Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Imputer - hardware issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "# imputer.fit_transform(X_train_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "# imp.fit(X_train_to_impute)\n",
    "# IterativeImputer(random_state=0)\n",
    "# Iterative_DataFrame = pd.DataFrame(imp.transform(X_train_to_impute))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "imp_mean.fit(X_train)\n",
    "X_train = pd.DataFrame(imp_mean.transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imp_mean.transform(X_test), columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer - median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_median = SimpleImputer(strategy='median')\n",
    "# imp_median.fit(X_train_to_impute)\n",
    "# Median_DataFrame = pd.DataFrame(imp_median.transform(X_train_to_impute))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Standarization - change the values so that the distribution standard deviation from the mean equals one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Standarize - MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_scaler = MinMaxScaler()\n",
    "# m_scaler.fit(Iterative_DataFrame)\n",
    "# m_scaler.transform(Iterative_DataFrame)\n",
    "# X_train_Iterative = pd.DataFrame(m_scaler.transform(Iterative_DataFrame))\n",
    "\n",
    "m_scaler = MinMaxScaler()\n",
    "m_scaler.fit(X_train)\n",
    "m_scaler.transform(X_train)\n",
    "X_train = pd.DataFrame(m_scaler.transform(X_train), columns=X_train.columns)\n",
    "\n",
    "m_scaler = MinMaxScaler()\n",
    "m_scaler.fit(X_test)\n",
    "m_scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(m_scaler.transform(X_test), columns=X_train.columns)\n",
    "\n",
    "# m_scaler = MinMaxScaler()\n",
    "# m_scaler.fit(Median_DataFrame)\n",
    "# X_train_Median = pd.DataFrame(m_scaler.transform(Median_DataFrame))\n",
    "\n",
    "# print(X_train_Iterative.shape)\n",
    "# print(X_train_Median.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Building neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20, input_shape = (135,), activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(80, activation = 'relu'))\n",
    "model.add(Dense(60, activation = 'relu'))\n",
    "model.add(Dense(40, activation = 'relu'))\n",
    "model.add(Dense(20, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[k_metrics.mae,\n",
    "                      ])\n",
    "\n",
    "X_val = X_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plotting loss and mae change over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# dims = (15.2, 10.75)\n",
    "fig = plt.figure(figsize=(15.2, 10.75))\n",
    "\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.clf()\n",
    "# # print(history_dict.keys())\n",
    "mae = history_dict['mean_absolute_error']\n",
    "val_mae = history_dict['val_mean_absolute_error']\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(epochs, mae, 'bo', label='Training mean absolute error')\n",
    "plt.plot(epochs, mae, 'b', label='Validation mean absolute error')\n",
    "\n",
    "plt.title('Training and validation mean absolute error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(r\"C:\\Users\\Acer\\Desktop\\change.png\")\n",
    "\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Function for predicting classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def predict_cls(arr, threshold):\n",
    "    \"\"\"\n",
    "    Predicting classes based on given probability threshold\n",
    "    \"\"\"\n",
    "    predicted = [0 if probability < threshold else 1 for probability in arr]\n",
    "    return predicted\n",
    "\n",
    "y_test = y_test.astype(int).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Confusion matrix for 0.5 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "my_y_pred = predict_cls(model.predict(X_test), 0.5)\n",
    "y_pred = model.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, my_y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "tp = conf_mat[0][0]\n",
    "tn = conf_mat[1][1]\n",
    "fp = conf_mat[0][1]\n",
    "fn = conf_mat[1][0]\n",
    "\n",
    "TPR_sensitivity = tp / (tp + fn)\n",
    "SPC_specificity = tn / (fp + tn)\n",
    "FPR = fp / (fp + tn)\n",
    "FDR = fp / (fp + tp)\n",
    "\n",
    "print(\"SENSITIVITY:\", TPR_sensitivity)\n",
    "print(\"SPECIFICITY:\", SPC_specificity)\n",
    "print(\"FPR:\", FPR)\n",
    "print(\"FDR:\", FDR)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15.2, 10.75))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
